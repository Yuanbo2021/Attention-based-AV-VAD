<h1 align="center">Attention-based cross-modal fusion for audio-visual voice activity detection in musical video streams<p></p></h1>

<h2 align="center">Demos of the detection results<p></p></h2>

<p align="center">Here are the detection results based on proposed attention-based AVVAD (ATT-AVVAD) framework in the paper.

<br>

The font on the top left of the video shows the activity of the anchor at the current moment. The anchor speaks, it shows speech; the anchor sings, it shows singing; the anchor has no action and there is sound in the background, it shows silence; otherwise it shows others.
</p>


<p>
<table class="tab">
<tr  align="center">
 
<td><video width=100%/ controls>
<source src="Video_demos/video_demo1.mp4" type="video/mp4">   
</video></td>    
 
<td><video width=100%/ controls>
<source src="Video_demos/video_demo2.mp4" type="video/mp4">   
</video></td> 
 
<td><video width=75%/ controls>
<source src="Video_demos/video_demo3.mp4" type="video/mp4">   
</video></td> 
 
<td><video width=100%/ controls>
<source src="Video_demos/video_demo4.mp4" type="video/mp4">   
</video></td> 

</tr>
</table>
</p>

<br>
<h3 align="center"><a name="part3">The proposed attention-based AVVAD (ATT-AVVAD) framework.</a><p></p></h3> 

<p><div align="center">
<img src="AVVAD-framework.PNG" width=75%/>
</div>

<p align="center"> The proposed attention-based AVVAD (ATT-AVVAD) framework consists of the audio-based module (audio branch), image-based module (visual branch), and attention-based fusion module. The audio-based module produces acoustic representation vectors for four target audio events: Silence, Speech of the anchor, Singing voice of the anchor, and Others. The image-based module aims to obtain the possibility of anchor vocalization based on facial parameters. Finally, we propose an attention-based module to fuse audio-visual information to comprehensively consider the bi-modal information to make final decisions at the audio-visual level.</p>

</p>

<br>
<h3 align="center">Visualization of core representation vectors distribution after attention-based fusion from a test sample using t-SNE.<p></p></h3> 

<p align="center"> The vectors in subgraph (a) are from the audio branch, vectors in subgraph (b) are from audio-visual modules after attention-based fusion.</p>

<p><div align="center"> 
<h3> Sample 1 </h3> <img src="after-attention-sample0.PNG"  width=60%/>
<br>

<h3> Sample 2 </h3> <img src="after-attention-sample1.PNG"  width=60%/>
<br>

<h3> Sample 3 </h3> <img src="after-attention-sample2.PNG"  width=60%/>
<br>

<h3> Sample 4 </h3> <img src="after-attention-sample3.PNG"  width=60%/>
</div>
</p>

<br>
<h3 align="center">Visualization of acoustic representation vectors and visual vocalization vector distribution from a test sample using t-SNE.<p></p></h3> 

<p align="center"> The vector (black dots) representing the vocalizing of the anchor is distributed on the side representing the voices of the anchor (green dots for singing, red dots for speech).</p>


<p><div align="center"><h3> Sample 5 </h3> <img src="audio-visual0.PNG" width=50%/></div></p>
<br>

<p><div align="center"><h3> Sample 6 </h3> <img src="audio-visual1.png" width=50%/></div></p>
<br>

<p><div align="center"><h3> Sample 7 </h3> <img src="audio-visual2.png" width=50%/></div></p>


<br>
<h3 align="center">Source code, please see <a href="https://github.com/Yuanbo2021/Attention-based-audio-visual-VAD/tree/main/Code" 
target="https://github.com/Yuanbo2021/Attention-based-audio-visual-VAD/tree/main/Code">here</a>.<p></p></h3>



